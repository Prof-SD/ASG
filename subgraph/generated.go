// Code generated by sparkle. DO NOT EDIT.

package subgraph

import (
	"bytes"
	"fmt"
	"math/big"
	"reflect"
	"time"

	eth "github.com/streamingfast/eth-go"
	"github.com/streamingfast/sparkle/entity"
	pbcodec "github.com/streamingfast/sparkle/pb/sf/ethereum/codec/v1"
	"github.com/streamingfast/sparkle/subgraph"
)

const (
	FactoryAddress = "0x0000000000000000000000000000000000000001"
	ZeroAddress    = "0x0000000000000000000000000000000000000000"
)

var (
	FactoryAddressBytes = eth.MustNewAddress(FactoryAddress).Bytes()
	ZeroAddressBytes    = eth.MustNewAddress(ZeroAddress).Bytes()
)

// Aliases for numerical functions
var (
	S  = entity.S
	B  = entity.B
	F  = entity.NewFloat
	FL = entity.NewFloatFromLiteral
	I  = entity.NewInt
	IL = entity.NewIntFromLiteral
	bf = func() *big.Float { return new(big.Float) }
	bi = func() *big.Int { return new(big.Int) }
)

var Definition = &subgraph.Definition{
	PackageName:         "subgraph",
	HighestParallelStep: 0,
	StartBlock:          0,
	IncludeFilter:       "",
	Entities: entity.NewRegistry(
		&Block{},
	),
	DDL: ddl,
	Manifest: `specVersion: 0.0.2
description: Apeswap record of blocks
repository: https://github.com/apeswapfinance/graphs
schema:
  file: ./schema.graphql
dataSources:
  - kind: ethereum/contract
    name: BSCBlocks
    network: mainnet
    source:
      address: "0x0000000000000000000000000000000000000001"
      abi: abi
    mapping:
      kind: ethereum/events
      apiVersion: 0.0.5
      language: wasm/assemblyscript
      entities:
        - Block
      abis:
        - name: abi
          file: ./abis/abi.json
      blockHandlers:
        - handler: handleBlock
      file: ./mappings/mapping.ts`,
	GraphQLSchema: `type Block @entity {
  id: ID!
  number: BigInt!
  timestamp: BigInt!
  parentHash: String
  author: String
  difficulty: BigInt
  totalDifficulty: BigInt
  gasUsed: BigInt
  gasLimit: BigInt
  receiptsRoot: String
  transactionsRoot: String
  stateRoot: String
  size: BigInt
  unclesHash: String
}`,
	Abis: map[string]string{
		"abi": `[
  {
    "constant": true,
    "inputs": [{ "name": "", "type": "address" }],
    "name": "tokenTable",
    "outputs": [
      { "name": "valid", "type": "bool" },
      { "name": "index", "type": "uint256" }
    ],
    "payable": false,
    "stateMutability": "view",
    "type": "function"
  },
  {
    "constant": false,
    "inputs": [
      { "name": "_token", "type": "address" },
      { "name": "_index", "type": "uint32" }
    ],
    "name": "unregisterConverter",
    "outputs": [],
    "payable": false,
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "constant": true,
    "inputs": [{ "name": "_token", "type": "address" }],
    "name": "latestConverterAddress",
    "outputs": [{ "name": "", "type": "address" }],
    "payable": false,
    "stateMutability": "view",
    "type": "function"
  },
  {
    "constant": true,
    "inputs": [{ "name": "", "type": "uint256" }],
    "name": "tokens",
    "outputs": [{ "name": "", "type": "address" }],
    "payable": false,
    "stateMutability": "view",
    "type": "function"
  },
  {
    "constant": false,
    "inputs": [],
    "name": "acceptOwnership",
    "outputs": [],
    "payable": false,
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "constant": true,
    "inputs": [],
    "name": "owner",
    "outputs": [{ "name": "", "type": "address" }],
    "payable": false,
    "stateMutability": "view",
    "type": "function"
  },
  {
    "constant": true,
    "inputs": [
      { "name": "_token", "type": "address" },
      { "name": "_index", "type": "uint32" }
    ],
    "name": "converterAddress",
    "outputs": [{ "name": "", "type": "address" }],
    "payable": false,
    "stateMutability": "view",
    "type": "function"
  },
  {
    "constant": true,
    "inputs": [{ "name": "_converter", "type": "address" }],
    "name": "tokenAddress",
    "outputs": [{ "name": "", "type": "address" }],
    "payable": false,
    "stateMutability": "view",
    "type": "function"
  },
  {
    "constant": true,
    "inputs": [],
    "name": "tokenCount",
    "outputs": [{ "name": "", "type": "uint256" }],
    "payable": false,
    "stateMutability": "view",
    "type": "function"
  },
  {
    "constant": true,
    "inputs": [{ "name": "_token", "type": "address" }],
    "name": "converterCount",
    "outputs": [{ "name": "", "type": "uint256" }],
    "payable": false,
    "stateMutability": "view",
    "type": "function"
  },
  {
    "constant": false,
    "inputs": [
      { "name": "_token", "type": "address" },
      { "name": "_converter", "type": "address" }
    ],
    "name": "registerConverter",
    "outputs": [],
    "payable": false,
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "constant": true,
    "inputs": [],
    "name": "newOwner",
    "outputs": [{ "name": "", "type": "address" }],
    "payable": false,
    "stateMutability": "view",
    "type": "function"
  },
  {
    "constant": false,
    "inputs": [{ "name": "_newOwner", "type": "address" }],
    "name": "transferOwnership",
    "outputs": [],
    "payable": false,
    "stateMutability": "nonpayable",
    "type": "function"
  },
  {
    "inputs": [],
    "payable": false,
    "stateMutability": "nonpayable",
    "type": "constructor"
  },
  {
    "anonymous": false,
    "inputs": [{ "indexed": true, "name": "_token", "type": "address" }],
    "name": "TokenAddition",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [{ "indexed": true, "name": "_token", "type": "address" }],
    "name": "TokenRemoval",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      { "indexed": true, "name": "_token", "type": "address" },
      { "indexed": false, "name": "_address", "type": "address" }
    ],
    "name": "ConverterAddition",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      { "indexed": true, "name": "_token", "type": "address" },
      { "indexed": false, "name": "_address", "type": "address" }
    ],
    "name": "ConverterRemoval",
    "type": "event"
  },
  {
    "anonymous": false,
    "inputs": [
      { "indexed": true, "name": "_prevOwner", "type": "address" },
      { "indexed": true, "name": "_newOwner", "type": "address" }
    ],
    "name": "OwnerUpdate",
    "type": "event"
  }
]`,
	},
	New: func(base subgraph.Base) subgraph.Subgraph {
		return &Subgraph{
			Base:               base,
			DynamicDataSources: map[string]*DynamicDataSourceXXX{},
		}
	},
	MergeFunc: func(step int, cached, new entity.Interface) entity.Interface {
		switch new.(type) {
		case interface {
			Merge(step int, new *Block)
		}:
			var c *Block
			if cached == nil {
				return new.(*Block)
			}
			c = cached.(*Block)
			el := new.(*Block)
			el.Merge(step, c)
			return el
		}
		panic("unsupported merge type")
	},
}

type Subgraph struct {
	subgraph.Base

	CurrentBlockDynamicDataSources map[string]*DynamicDataSourceXXX
	DynamicDataSources             map[string]*DynamicDataSourceXXX
}

// Block
type Block struct {
	entity.Base
	Number           entity.Int  `db:"number" csv:"number"`
	Timestamp        entity.Int  `db:"timestamp" csv:"timestamp"`
	ParentHash       *string     `db:"parent_hash,nullable" csv:"parent_hash"`
	Author           *string     `db:"author,nullable" csv:"author"`
	Difficulty       *entity.Int `db:"difficulty,nullable" csv:"difficulty"`
	TotalDifficulty  *entity.Int `db:"total_difficulty,nullable" csv:"total_difficulty"`
	GasUsed          *entity.Int `db:"gas_used,nullable" csv:"gas_used"`
	GasLimit         *entity.Int `db:"gas_limit,nullable" csv:"gas_limit"`
	ReceiptsRoot     *string     `db:"receipts_root,nullable" csv:"receipts_root"`
	TransactionsRoot *string     `db:"transactions_root,nullable" csv:"transactions_root"`
	StateRoot        *string     `db:"state_root,nullable" csv:"state_root"`
	Size             *entity.Int `db:"size,nullable" csv:"size"`
	UnclesHash       *string     `db:"uncles_hash,nullable" csv:"uncles_hash"`
}

func NewBlock(id string) *Block {
	return &Block{
		Base:      entity.NewBase(id),
		Number:    IL(0),
		Timestamp: IL(0),
	}
}

func (_ *Block) SkipDBLookup() bool {
	return false
}

func (s *Subgraph) HandleBlock(block *pbcodec.Block) error {
	idx := uint32(0)
	s.CurrentBlockDynamicDataSources = make(map[string]*DynamicDataSourceXXX)

	for _, trace := range block.TransactionTraces {
		logs := trace.Logs()
		for _, log := range logs {
			var ethLog interface{} = log
			eventLog := codecLogToEthLog(ethLog.(*pbcodec.Log), idx)
			idx++
			if bytes.Equal(FactoryAddressBytes, log.Address) {
				ev, err := DecodeEvent(eventLog, block, trace)
				if err != nil {
					return fmt.Errorf("parsing event: %w", err)
				}
				if err := s.HandleEvent(ev); err != nil {
					return fmt.Errorf("handling event: %w", err)
				}

			}
		}
	}

	if len(s.CurrentBlockDynamicDataSources) == 0 {
		return nil
	}

	for _, trace := range block.TransactionTraces {
		logs := trace.Logs()
		for _, log := range logs {
			var ethLog interface{} = log
			eventLog := codecLogToEthLog(ethLog.(*pbcodec.Log), idx)
			idx++
			if s.IsCurrentDynamicDataSource(eth.Address(log.Address).Pretty()) {
				ev, err := DecodeEvent(eventLog, block, trace)
				if err != nil {
					return fmt.Errorf("parsing event: %w", err)
				}
				if err := s.HandleEvent(ev); err != nil {
					return fmt.Errorf("handling event: %w", err)
				}

			}
		}
	}

	for k, v := range s.CurrentBlockDynamicDataSources {
		s.DynamicDataSources[k] = v
	}

	return nil
}
func (s *Subgraph) HandleEvent(ev interface{}) error {
	switch e := ev.(type) {

	}

	return nil
}

func codecLogToEthLog(l *pbcodec.Log, idx uint32) *eth.Log {
	return &eth.Log{
		Address:    l.Address,
		Topics:     l.Topics,
		Data:       l.Data,
		Index:      l.Index,
		BlockIndex: idx,
	}
}

// abi
// abiConverterAddition event

type abiConverterAdditionEvent struct {
	*entity.BaseEvent
	LogAddress eth.Address
	LogIndex   int

	// Fields
	Token   eth.Address `eth:",indexed"`
	Address eth.Address `eth:""`
}

var hashabiConverterAdditionEvent = eth.Keccak256([]byte("ConverterAddition(address,address)"))

func IsabiConverterAdditionEvent(log *eth.Log) bool {
	return bytes.Equal(log.Topics[0], hashabiConverterAdditionEvent)
}

func NewabiConverterAdditionEvent(log *eth.Log, block *pbcodec.Block, trace *pbcodec.TransactionTrace) (*abiConverterAdditionEvent, error) {
	var err error
	ev := &abiConverterAdditionEvent{
		BaseEvent:  &entity.BaseEvent{},
		LogAddress: log.Address,
		LogIndex:   int(log.BlockIndex),
	}

	ev.SetBlockAndTransaction(block, trace)

	dec := eth.NewLogDecoder(log)
	if _, err := dec.ReadTopic(); err != nil {
		return nil, fmt.Errorf("reading topic 0: %w", err)
	}
	f0, err := dec.ReadTypedTopic("address")
	if err != nil {
		return nil, fmt.Errorf("reading _token: %w", err)
	}
	ev.Token = f0.(eth.Address)
	ev.Address, err = dec.DataDecoder.ReadAddress()
	if err != nil {
		return nil, fmt.Errorf("reading _address:  %w", err)
	}
	return ev, nil
}

// abiConverterRemoval event

type abiConverterRemovalEvent struct {
	*entity.BaseEvent
	LogAddress eth.Address
	LogIndex   int

	// Fields
	Token   eth.Address `eth:",indexed"`
	Address eth.Address `eth:""`
}

var hashabiConverterRemovalEvent = eth.Keccak256([]byte("ConverterRemoval(address,address)"))

func IsabiConverterRemovalEvent(log *eth.Log) bool {
	return bytes.Equal(log.Topics[0], hashabiConverterRemovalEvent)
}

func NewabiConverterRemovalEvent(log *eth.Log, block *pbcodec.Block, trace *pbcodec.TransactionTrace) (*abiConverterRemovalEvent, error) {
	var err error
	ev := &abiConverterRemovalEvent{
		BaseEvent:  &entity.BaseEvent{},
		LogAddress: log.Address,
		LogIndex:   int(log.BlockIndex),
	}

	ev.SetBlockAndTransaction(block, trace)

	dec := eth.NewLogDecoder(log)
	if _, err := dec.ReadTopic(); err != nil {
		return nil, fmt.Errorf("reading topic 0: %w", err)
	}
	f0, err := dec.ReadTypedTopic("address")
	if err != nil {
		return nil, fmt.Errorf("reading _token: %w", err)
	}
	ev.Token = f0.(eth.Address)
	ev.Address, err = dec.DataDecoder.ReadAddress()
	if err != nil {
		return nil, fmt.Errorf("reading _address:  %w", err)
	}
	return ev, nil
}

// abiOwnerUpdate event

type abiOwnerUpdateEvent struct {
	*entity.BaseEvent
	LogAddress eth.Address
	LogIndex   int

	// Fields
	PrevOwner eth.Address `eth:",indexed"`
	NewOwner  eth.Address `eth:",indexed"`
}

var hashabiOwnerUpdateEvent = eth.Keccak256([]byte("OwnerUpdate(address,address)"))

func IsabiOwnerUpdateEvent(log *eth.Log) bool {
	return bytes.Equal(log.Topics[0], hashabiOwnerUpdateEvent)
}

func NewabiOwnerUpdateEvent(log *eth.Log, block *pbcodec.Block, trace *pbcodec.TransactionTrace) (*abiOwnerUpdateEvent, error) {
	var err error
	ev := &abiOwnerUpdateEvent{
		BaseEvent:  &entity.BaseEvent{},
		LogAddress: log.Address,
		LogIndex:   int(log.BlockIndex),
	}

	ev.SetBlockAndTransaction(block, trace)

	dec := eth.NewLogDecoder(log)
	if _, err := dec.ReadTopic(); err != nil {
		return nil, fmt.Errorf("reading topic 0: %w", err)
	}
	f0, err := dec.ReadTypedTopic("address")
	if err != nil {
		return nil, fmt.Errorf("reading _prevOwner: %w", err)
	}
	ev.PrevOwner = f0.(eth.Address)
	f1, err := dec.ReadTypedTopic("address")
	if err != nil {
		return nil, fmt.Errorf("reading _newOwner: %w", err)
	}
	ev.NewOwner = f1.(eth.Address)
	return ev, nil
}

// abiTokenAddition event

type abiTokenAdditionEvent struct {
	*entity.BaseEvent
	LogAddress eth.Address
	LogIndex   int

	// Fields
	Token eth.Address `eth:",indexed"`
}

var hashabiTokenAdditionEvent = eth.Keccak256([]byte("TokenAddition(address)"))

func IsabiTokenAdditionEvent(log *eth.Log) bool {
	return bytes.Equal(log.Topics[0], hashabiTokenAdditionEvent)
}

func NewabiTokenAdditionEvent(log *eth.Log, block *pbcodec.Block, trace *pbcodec.TransactionTrace) (*abiTokenAdditionEvent, error) {
	var err error
	ev := &abiTokenAdditionEvent{
		BaseEvent:  &entity.BaseEvent{},
		LogAddress: log.Address,
		LogIndex:   int(log.BlockIndex),
	}

	ev.SetBlockAndTransaction(block, trace)

	dec := eth.NewLogDecoder(log)
	if _, err := dec.ReadTopic(); err != nil {
		return nil, fmt.Errorf("reading topic 0: %w", err)
	}
	f0, err := dec.ReadTypedTopic("address")
	if err != nil {
		return nil, fmt.Errorf("reading _token: %w", err)
	}
	ev.Token = f0.(eth.Address)
	return ev, nil
}

// abiTokenRemoval event

type abiTokenRemovalEvent struct {
	*entity.BaseEvent
	LogAddress eth.Address
	LogIndex   int

	// Fields
	Token eth.Address `eth:",indexed"`
}

var hashabiTokenRemovalEvent = eth.Keccak256([]byte("TokenRemoval(address)"))

func IsabiTokenRemovalEvent(log *eth.Log) bool {
	return bytes.Equal(log.Topics[0], hashabiTokenRemovalEvent)
}

func NewabiTokenRemovalEvent(log *eth.Log, block *pbcodec.Block, trace *pbcodec.TransactionTrace) (*abiTokenRemovalEvent, error) {
	var err error
	ev := &abiTokenRemovalEvent{
		BaseEvent:  &entity.BaseEvent{},
		LogAddress: log.Address,
		LogIndex:   int(log.BlockIndex),
	}

	ev.SetBlockAndTransaction(block, trace)

	dec := eth.NewLogDecoder(log)
	if _, err := dec.ReadTopic(); err != nil {
		return nil, fmt.Errorf("reading topic 0: %w", err)
	}
	f0, err := dec.ReadTypedTopic("address")
	if err != nil {
		return nil, fmt.Errorf("reading _token: %w", err)
	}
	ev.Token = f0.(eth.Address)
	return ev, nil
}

func DecodeEvent(log *eth.Log, block *pbcodec.Block, trace *pbcodec.TransactionTrace) (interface{}, error) {

	if IsabiConverterAdditionEvent(log) {
		ev, err := NewabiConverterAdditionEvent(log, block, trace)
		if err != nil {
			return nil, fmt.Errorf("decoding abiConverterAddition event: %w", err)
		}
		return ev, nil
	}
	if IsabiConverterRemovalEvent(log) {
		ev, err := NewabiConverterRemovalEvent(log, block, trace)
		if err != nil {
			return nil, fmt.Errorf("decoding abiConverterRemoval event: %w", err)
		}
		return ev, nil
	}
	if IsabiOwnerUpdateEvent(log) {
		ev, err := NewabiOwnerUpdateEvent(log, block, trace)
		if err != nil {
			return nil, fmt.Errorf("decoding abiOwnerUpdate event: %w", err)
		}
		return ev, nil
	}
	if IsabiTokenAdditionEvent(log) {
		ev, err := NewabiTokenAdditionEvent(log, block, trace)
		if err != nil {
			return nil, fmt.Errorf("decoding abiTokenAddition event: %w", err)
		}
		return ev, nil
	}
	if IsabiTokenRemovalEvent(log) {
		ev, err := NewabiTokenRemovalEvent(log, block, trace)
		if err != nil {
			return nil, fmt.Errorf("decoding abiTokenRemoval event: %w", err)
		}
		return ev, nil
	}

	return nil, nil
}

func (s *Subgraph) IsDynamicDataSource(address string) bool {
	_, ok := s.DynamicDataSources[address]
	return ok
}

func (s *Subgraph) IsCurrentDynamicDataSource(address string) bool {
	_, ok := s.CurrentBlockDynamicDataSources[address]
	return ok
}

func (s *Subgraph) LoadDynamicDataSources(blockNum uint64) error {
	return nil
}

type DDL struct {
	createTables map[string]string
	indexes      map[string][]*index
	schemaSetup  string
}

var ddl *DDL

type index struct {
	createStatement string
	dropStatement   string
}

var createTables = map[string]string{}
var indexes = map[string][]*index{}

func init() {
	ddl = &DDL{
		createTables: map[string]string{},
		indexes:      map[string][]*index{},
	}

	Definition.DDL = ddl

	ddl.createTables["block"] = `
create table if not exists %%SCHEMA%%.block
(
	id text not null,

	"number" numeric not null,

	"timestamp" numeric not null,

	"parent_hash" text,

	"author" text,

	"difficulty" numeric,

	"total_difficulty" numeric,

	"gas_used" numeric,

	"gas_limit" numeric,

	"receipts_root" text,

	"transactions_root" text,

	"state_root" text,

	"size" numeric,

	"uncles_hash" text,

	vid bigserial not null constraint block_pkey primary key,
	block_range int4range not null,
	_updated_block_number numeric not null
);

alter table %%SCHEMA%%.block owner to graph;
alter sequence %%SCHEMA%%.block_vid_seq owned by %%SCHEMA%%.block.vid;
alter table only %%SCHEMA%%.block alter column vid SET DEFAULT nextval('%%SCHEMA%%.block_vid_seq'::regclass);
`

	ddl.indexes["block"] = func() []*index {
		var indexes []*index
		indexes = append(indexes, &index{
			createStatement: `create index if not exists block_block_range_closed on %%SCHEMA%%.block (COALESCE(upper(block_range), 2147483647)) where (COALESCE(upper(block_range), 2147483647) < 2147483647);`,
			dropStatement:   `drop index if exists %%SCHEMA%%.block_block_range_closed;`,
		})
		indexes = append(indexes, &index{
			createStatement: `create index if not exists block_id on %%SCHEMA%%.block (id);`,
			dropStatement:   `drop index if exists %%SCHEMA%%.block_id;`,
		})
		indexes = append(indexes, &index{
			createStatement: `create index if not exists block_updated_block_number on %%SCHEMA%%.block (_updated_block_number);`,
			dropStatement:   `drop index if exists %%SCHEMA%%.block_updated_block_number;`,
		})

		indexes = append(indexes, &index{
			createStatement: `create index if not exists block_id_block_range_fake_excl on %%SCHEMA%%.block using gist (block_range, id);`,
			dropStatement:   `drop index if exists %%SCHEMA%%.block_id_block_range_fake_excl;`,
		})

		indexes = append(indexes, &index{
			createStatement: `create index if not exists block_number on %%SCHEMA%%.block using btree ("number");`,
			dropStatement:   `drop index if exists %%SCHEMA%%.block_number;`,
		})

		indexes = append(indexes, &index{
			createStatement: `create index if not exists block_timestamp on %%SCHEMA%%.block using btree ("timestamp");`,
			dropStatement:   `drop index if exists %%SCHEMA%%.block_timestamp;`,
		})

		indexes = append(indexes, &index{
			createStatement: `create index if not exists block_parent_hash on %%SCHEMA%%.block ("left"("parent_hash", 256));`,
			dropStatement:   `drop index if exists %%SCHEMA%%.block_parent_hash;`,
		})

		indexes = append(indexes, &index{
			createStatement: `create index if not exists block_author on %%SCHEMA%%.block ("left"("author", 256));`,
			dropStatement:   `drop index if exists %%SCHEMA%%.block_author;`,
		})

		indexes = append(indexes, &index{
			createStatement: `create index if not exists block_difficulty on %%SCHEMA%%.block using btree ("difficulty");`,
			dropStatement:   `drop index if exists %%SCHEMA%%.block_difficulty;`,
		})

		indexes = append(indexes, &index{
			createStatement: `create index if not exists block_total_difficulty on %%SCHEMA%%.block using btree ("total_difficulty");`,
			dropStatement:   `drop index if exists %%SCHEMA%%.block_total_difficulty;`,
		})

		indexes = append(indexes, &index{
			createStatement: `create index if not exists block_gas_used on %%SCHEMA%%.block using btree ("gas_used");`,
			dropStatement:   `drop index if exists %%SCHEMA%%.block_gas_used;`,
		})

		indexes = append(indexes, &index{
			createStatement: `create index if not exists block_gas_limit on %%SCHEMA%%.block using btree ("gas_limit");`,
			dropStatement:   `drop index if exists %%SCHEMA%%.block_gas_limit;`,
		})

		indexes = append(indexes, &index{
			createStatement: `create index if not exists block_receipts_root on %%SCHEMA%%.block ("left"("receipts_root", 256));`,
			dropStatement:   `drop index if exists %%SCHEMA%%.block_receipts_root;`,
		})

		indexes = append(indexes, &index{
			createStatement: `create index if not exists block_transactions_root on %%SCHEMA%%.block ("left"("transactions_root", 256));`,
			dropStatement:   `drop index if exists %%SCHEMA%%.block_transactions_root;`,
		})

		indexes = append(indexes, &index{
			createStatement: `create index if not exists block_state_root on %%SCHEMA%%.block ("left"("state_root", 256));`,
			dropStatement:   `drop index if exists %%SCHEMA%%.block_state_root;`,
		})

		indexes = append(indexes, &index{
			createStatement: `create index if not exists block_size on %%SCHEMA%%.block using btree ("size");`,
			dropStatement:   `drop index if exists %%SCHEMA%%.block_size;`,
		})

		indexes = append(indexes, &index{
			createStatement: `create index if not exists block_uncles_hash on %%SCHEMA%%.block ("left"("uncles_hash", 256));`,
			dropStatement:   `drop index if exists %%SCHEMA%%.block_uncles_hash;`,
		})

		return indexes
	}()
	ddl.schemaSetup = `
CREATE SCHEMA if not exists %%SCHEMA%%;
DO
$do$
    BEGIN
        IF NOT EXISTS (
                SELECT FROM pg_catalog.pg_roles  -- SELECT list can be empty for this
                WHERE  rolname = 'graph') THEN
            CREATE ROLE graph;
        END IF;
    END
$do$;

set statement_timeout = 0;
set idle_in_transaction_session_timeout = 0;
set client_encoding = 'UTF8';
set standard_conforming_strings = on;
select pg_catalog.set_config('search_path', '', false);
set check_function_bodies = false;
set xmloption = content;
set client_min_messages = warning;
set row_security = off;

create extension if not exists btree_gist with schema %%SCHEMA%%;


create table if not exists %%SCHEMA%%.cursor
(
	id integer not null
		constraint cursor_pkey
			primary key,
	cursor text
);
alter table %%SCHEMA%%.cursor owner to graph;

create table %%SCHEMA%%.poi2$
(
    digest      bytea     not null,
    id          text      not null,
    vid         bigserial not null
        constraint poi2$_pkey
            primary key,
    block_range int4range not null,
	_updated_block_number  numeric not null,
    constraint poi2$_id_block_range_excl
        exclude using gist (id with =, block_range with &&)
);

alter table %%SCHEMA%%.poi2$
    owner to graph;

create index brin_poi2$
    on %%SCHEMA%%.poi2$ using brin (lower(block_range), COALESCE(upper(block_range), 2147483647), vid);

CREATE INDEX poi2$_updated_block_number
    ON %%SCHEMA%%.poi2$ USING btree
	(_updated_block_number ASC NULLS LAST)
	TABLESPACE pg_default;

create index poi2$_block_range_closed
    on %%SCHEMA%%.poi2$ (COALESCE(upper(block_range), 2147483647))
    where (COALESCE(upper(block_range), 2147483647) < 2147483647);

create index attr_12_0_poi2$_digest
    on %%SCHEMA%%.poi2$ (digest);

create index attr_12_1_poi2$_id
    on %%SCHEMA%%.poi2$ ("left"(id, 256));

create table if not exists %%SCHEMA%%.dynamic_data_source_xxx
(
	id text not null,
	context text not null,
	abi text not null,
	vid bigserial not null
		constraint dynamic_data_source_xxx_pkey
			primary key,
	block_range int4range not null,
	_updated_block_number numeric not null
);

alter table %%SCHEMA%%.dynamic_data_source_xxx owner to graph;

create index if not exists dynamic_data_source_xxx_block_range_closed
	on %%SCHEMA%%.dynamic_data_source_xxx (COALESCE(upper(block_range), 2147483647))
	where (COALESCE(upper(block_range), 2147483647) < 2147483647);

create index if not exists dynamic_data_source_xxx_id
	on %%SCHEMA%%.dynamic_data_source_xxx (id);

create index if not exists dynamic_data_source_xxx_abi
	on %%SCHEMA%%.dynamic_data_source_xxx (abi);

`

}

func (d *DDL) InitiateSchema(handleStatement func(statement string) error) error {
	err := handleStatement(d.schemaSetup)
	if err != nil {
		return fmt.Errorf("handle statement: %w", err)
	}
	return nil
}

func (d *DDL) CreateTables(handleStatement func(table string, statement string) error) error {
	for table, statement := range d.createTables {
		err := handleStatement(table, statement)
		if err != nil {
			return fmt.Errorf("handle statement: %w", err)
		}
	}
	return nil
}

func (d *DDL) CreateIndexes(handleStatement func(table string, statement string) error) error {
	for table, idxs := range d.indexes {
		for _, idx := range idxs {
			err := handleStatement(table, idx.createStatement)
			if err != nil {
				return fmt.Errorf("handle statement: %w", err)
			}
		}
	}
	return nil
}

func (d *DDL) DropIndexes(handleStatement func(table string, statement string) error) error {
	for table, idxs := range d.indexes {
		for _, idx := range idxs {
			err := handleStatement(table, idx.dropStatement)
			if err != nil {
				return fmt.Errorf("handle statement: %w", err)
			}
		}
	}
	return nil
}

func NewTestSubgraph(int subgraph.Intrinsics) *Subgraph {
	return &Subgraph{
		Base: subgraph.Base{
			Intrinsics: int,
			Definition: Definition,
			ID:         "testSubgraph",
			Log:        zlog,
		},
	}
}

type TestIntrinsics struct {
	store map[string]map[string]entity.Interface
	step  int
}

func NewTestIntrinsics(testCase *TestCase) *TestIntrinsics {
	i := &TestIntrinsics{
		store: make(map[string]map[string]entity.Interface),
		step:  99999,
	}

	if testCase != nil {
		i.initialize(testCase)
	}

	return i
}

func (i *TestIntrinsics) initialize(testCase *TestCase) {
	i.setStoreData(testCase.StoreData)
}

func (i *TestIntrinsics) setStoreData(ents []*TypedEntity) {
	for _, value := range ents {
		err := i.Save(value.Entity)
		if err != nil {
			panic(err)
		}
	}
}

func (i *TestIntrinsics) Save(e entity.Interface) error {
	tableName := entity.GetTableName(e)
	tbl, found := i.store[tableName]
	if !found {
		tbl = make(map[string]entity.Interface)
		i.store[tableName] = tbl
	}

	e.SetExists(true)
	e.SetMutated(i.step)

	tbl[e.GetID()] = e
	return nil
}

func (i *TestIntrinsics) Load(e entity.Interface) error {
	tableName := entity.GetTableName(e)
	tbl, found := i.store[tableName]
	if !found {
		return nil
	}

	id := e.GetID()
	cachedEntity, found := tbl[id]
	if found {
		if cachedEntity == nil {
			return nil
		}
		ve := reflect.ValueOf(e).Elem()
		ve.Set(reflect.ValueOf(cachedEntity).Elem())
		return nil
	}

	return nil
}

func (i *TestIntrinsics) LoadAllDistinct(e entity.Interface, blockNum uint64) ([]entity.Interface, error) {
	result := make([]entity.Interface, 0)

	tableName := entity.GetTableName(e)
	tbl, found := i.store[tableName]
	if !found {
		return result, nil
	}

	for _, v := range tbl {
		result = append(result, v)
	}
	return result, nil
}

func (i *TestIntrinsics) Remove(e entity.Interface) error {
	tableName := entity.GetTableName(e)
	tbl, found := i.store[tableName]
	if !found {
		return nil
	}

	id := e.GetID()
	delete(tbl, id)
	return nil
}

func (i *TestIntrinsics) Block() subgraph.BlockRef {
	return &blockRef{
		id:        "0x1",
		num:       1,
		timestamp: time.Time{},
	}
}

func (i *TestIntrinsics) Step() int {
	return i.step
}

func (i *TestIntrinsics) StepBelow(step int) bool {
	return i.step < step
}

func (i *TestIntrinsics) StepAbove(step int) bool {
	return i.step > step
}

func (i *TestIntrinsics) RPC(calls []*subgraph.RPCCall) ([]*subgraph.RPCResponse, error) {
	return nil, nil
}

type TestCase struct {
	StoreData []*TypedEntity `yaml:"storeData" json:"storeData"`
	Events    []*TypedEvent  `yaml:"events" json:"events"`
}

type TypedEntity struct {
	Type   string
	Entity entity.Interface
}

func (t *TypedEntity) UnmarshalJSON(data []byte) error {
	s := &struct {
		Type   string          `json:"type" yaml:"type"`
		Entity json.RawMessage `json:"entity" yaml:"entity"`
	}{}

	err := json.Unmarshal(data, &s)
	if err != nil {
		return err
	}

	var ent entity.Interface
	switch s.Type {
	case "block":
		tempEnt := &Block{}
		err := json.Unmarshal(s.Entity, &tempEnt)
		if err != nil {
			return err
		}
		ent = tempEnt
	}

	t.Entity = ent
	t.Type = s.Type

	return nil
}

type TypedEvent struct {
	Type  string
	Event interface{}
}

func (t *TypedEvent) UnmarshalJSON(data []byte) error {
	s := &struct {
		Type  string
		Event json.RawMessage
	}{}

	err := json.Unmarshal(data, &s)
	if err != nil {
		return err
	}

	var event interface{}
	switch s.Type {
	case "abiConverterAdditionEvent":
		ev := &abiConverterAdditionEvent{}
		err := json.Unmarshal(s.Event, &ev)
		if err != nil {
			return err
		}
		event = ev
	case "abiConverterRemovalEvent":
		ev := &abiConverterRemovalEvent{}
		err := json.Unmarshal(s.Event, &ev)
		if err != nil {
			return err
		}
		event = ev
	case "abiOwnerUpdateEvent":
		ev := &abiOwnerUpdateEvent{}
		err := json.Unmarshal(s.Event, &ev)
		if err != nil {
			return err
		}
		event = ev
	case "abiTokenAdditionEvent":
		ev := &abiTokenAdditionEvent{}
		err := json.Unmarshal(s.Event, &ev)
		if err != nil {
			return err
		}
		event = ev
	case "abiTokenRemovalEvent":
		ev := &abiTokenRemovalEvent{}
		err := json.Unmarshal(s.Event, &ev)
		if err != nil {
			return err
		}
		event = ev
	}

	t.Event = event
	t.Type = s.Type

	return nil
}
